# Model Guided Research: Посібник для підприємства

*Документ для команди: що це за проект, навіщо він потрібен, і як почати з ним працювати*

---

## 1. Що це за проект (простими словами)

Сучасні AI-моделі (GPT, Claude, Gemini) побудовані на **трансформерах**. Серцевина трансформера - механізм **attention** (увага), який вирішує, на які частини тексту модель "дивиться" при обробці кожного слова.

Стандартний attention працює так: кожне слово порівнюється з кожним іншим через скалярний добуток, результати нормалізуються через softmax. Це просто, але не ідеально.

**Цей проект досліджує 11 альтернативних математичних підходів** до attention, щоб з'ясувати: чи можна зробити краще? Швидше? Надійніше? Економніше з пам'яттю?

Проект створений Джефрі Емануелом у співпраці з GPT-5 Pro за унікальною парадигмою: AI сам запропонував дослідницькі напрямки, сам оцінив їх перспективність, а людина валідувала та імплементувала найкращі.

---

## 2. Що є в репозиторії

### Два рівні реалізації

```
model_guided_research/
|
|-- *.py (корінь)         <-- JAX демо: 11 інтерактивних демонстрацій
|                             Мета: "чи працює математика в принципі?"
|                             Запуск: mgr run tropical
|
|-- nanochat/             <-- PyTorch production: повноцінний GPT-тренер
|                             Мета: "чи працює це при реальному тренуванні?"
|                             Запуск: python -m nanochat.train --attention-type tropical
|
|-- tests/                <-- 136 тестів: коректність, властивості, практична корисність
|-- docs/                 <-- Технічна документація
|-- markdown_documentation/  <-- Математичні основи кожного підходу
|-- artifacts/            <-- Результати експериментів (summary.json + run.md)
|-- cli.py                <-- Головний CLI (команда mgr)
```

### Архітектура моделі Nanochat

Nanochat - це сучасний GPT-трансформер з такими особливостями:

- **RoPE** (Rotary Positional Embeddings) - позиційне кодування через обертання
- **ReLU^2** - квадрат ReLU замість GELU в MLP-блоках
- **QK Norm** - нормалізація queries та keys
- **GQA** (Group-Query Attention) - різна кількість query та KV голів
- **Logit softcap** = 15 - обмеження логітів для стабільності
- **Без bias** у всіх лінійних шарах
- **RMSNorm** без параметрів, що навчаються

Дефолтна конфігурація: 12 шарів, 768 embedding, 6 голів, vocab 50304.

### Дані для тренування

За замовчуванням використовується **FineWeb-Edu** (100B токенів, освітні веб-тексти) з HuggingFace. Дані в форматі parquet, токенізація GPT-4 BPE на льоту.

Завантажуються автоматично: `--auto-download-data`.

### Оптимізатори (3 штуки)

| Оптимізатор | Суть | Коли використовувати |
|---|---|---|
| **AdamW** | Стандартний адаптивний оптимізатор | Базова лінія, надійний вибір |
| **Muon** | Ортогоналізація Ньютона-Шульца для матричних параметрів | Потенційно краща генералізація |
| **HOSS** | Другого порядку, через Hessian-vector products | Складні ландшафти loss, мало даних |

### Scheduler-и (2 штуки)

| Scheduler | Суть |
|---|---|
| **none** | Без розкладу LR (константа після warmup) |
| **ordinal** | Трансфінітні ординали - ієрархічні рестарти на основі EMA loss |

---

## 3. 11 типів Attention: що і навіщо

### Таблиця для швидкого розуміння

| # | Тип | Математика | Ключова перевага | Найкраще для |
|---|---|---|---|---|
| 1 | **standard** | softmax(QK^T/sqrt(d))V | Базова лінія | Загальне використання |
| 2 | **tropical** | max-plus алгебра замість sum-exp | Robustness: 1-Lipschitz, сертифіковані margin-и | Safety-critical, fraud detection |
| 3 | **ultrametric** | p-адична відстань, LCP-kernel | O(N log N) замість O(N^2) | Довгі послідовності, ієрархічні дані |
| 4 | **simplicial** | Симпліціальні комплекси, Hodge-розклад | k-way взаємодії (не лише пари) | Multi-entity reasoning, supply chain |
| 5 | **quaternion** | 4D гіперкомплексні числа (rotor-gate) | Вбудована rotation-invariance | 3D/геопросторові дані |
| 6 | **octonion** | 8D не-асоціативна алгебра (Кейлі-Діксон) | Найбагатша алгебраїчна структура | Багатовимірні обертання |
| 7 | **braid** | Групи кіс, Yang-Baxter рівняння | Топологічно захищена інформація | Послідовності, перестановки |
| 8 | **fractal** | IFS, ієрархічна маршрутизація | Стійкість до catastrophic forgetting | Рекурсивні/самоподібні дані |
| 9 | **surreal** | Сюрреальні числа Конвея | Мультимасштабні представлення | Широкий динамічний діапазон |
| 10 | **reversible** | Additive coupling, точна зворотність | O(1) пам'ять при тренуванні | Дуже глибокі мережі, edge-пристрої |
| 11 | **gauge** | Групи Лі SO(D), parallel transport | Стабільність градієнтів | Погано обумовлені задачі |

### Пояснення для не-математиків

**Tropical (тропічна геометрія):**
Замість "зважити все і усереднити" (softmax) - "вибрати найкращий варіант" (max). Як різниця між "середня оцінка ресторану" і "найкраща страва в меню". Дає гарантовану стійкість до шуму у вхідних даних.

**Ultrametric (ультраметрика):**
Організовує токени в дерево за схожістю. Замість того щоб кожен токен дивився на всі інші (N^2 операцій), він дивиться тільки на свою гілку дерева (N log N). Як різниця між тим, щоб обдзвонити всіх у компанії vs зателефонувати своєму менеджеру.

**Simplicial (симпліціальні комплекси):**
Стандартний attention бачить тільки пари слів. Simplicial бачить трійки, четвірки - групові взаємодії. "Банк видав кредит компанії" - це три сутності, пов'язані одночасно, не три окремі пари.

**Reversible (зворотні обчислення):**
Звичайна мережа зберігає проміжні результати для backpropagation - це багато пам'яті. Reversible може відновити проміжні результати з кінцевого стану, тому потрібна пам'ять тільки для одного шару.

**Quaternion (кватерніони):**
Числа, які природно описують обертання у 3D просторі. Якщо ваші дані мають геометричну структуру (координати, кути, орієнтації), кватерніонний attention "розуміє" її без додаткового навчання.

---

## 4. Як запускати

### Попередні вимоги

```bash
# Python 3.13+, uv package manager
# Встановити uv якщо ще немає:
curl -LsSf https://astral.sh/uv/install.sh | sh

# Клонувати та налаштувати:
git clone <repo-url>
cd model_guided_research
uv venv
source .venv/bin/activate
uv sync --extra dev

# Дозволити direnv (якщо використовується):
direnv allow
```

### JAX демо (дослідження математики)

```bash
# Список всіх демо:
mgr list

# Запустити конкретне демо:
mgr run tropical
mgr run ultrametric
mgr run matrix-gauge

# Запустити всі 11 демо:
mgr run-all
```

### Тренування моделі (PyTorch)

```bash
# Мінімальний тест (CPU, 20 кроків, автозавантаження даних):
python -m nanochat.train \
    --auto-download-data \
    --device cpu \
    --max-steps 20 \
    --batch-size 4 \
    --sequence-len 128 \
    --n-layer 2 --n-head 2 --n-kv-head 2 --n-embd 64

# Серйозне тренування (GPU):
python -m nanochat.train \
    --auto-download-data \
    --device cuda \
    --attention-type tropical \
    --optimizer-type muon \
    --batch-size 8 \
    --sequence-len 256 \
    --n-layer 4 --n-head 4 --n-kv-head 4 --n-embd 128 \
    --max-steps 500 \
    --learning-rate 6e-4

# Бенчмарк: порівняння кількох attention типів при однаковому compute budget:
mgr bench-fixed-flops \
    -a standard -a tropical -a ultrametric -a simplicial \
    --target-flops 2e9 \
    --device cuda \
    --auto-download-data
```

### Тести

```bash
# Всі тести (має бути 135 passed, 1 skipped):
python -m pytest tests/ -v

# Тільки математична коректність:
python -m pytest tests/test_mathematical_correctness.py -v

# Тільки практична корисність:
python -m pytest tests/test_practical_utility.py -v
```

---

## 5. Цілі для підприємства

### Початкові (тижні 1-4)

**Ціль 1: Побудова внутрішньої AI-експертизи**
- Команда проходить через JAX демо як навчальні лабораторні
- Інженери розуміють, чим відрізняються різні attention-механізми
- Результат: команда може оцінювати AI-продукти вендорів з розумінням архітектури

**Ціль 2: Базова лінія порівняння**
- Запуск bench-fixed-flops на всіх 11 типах
- Перша таблиця: який механізм дає найкращий loss за однаковий compute
- Результат: розуміння, які підходи взагалі "живі" для ваших задач

**Ціль 3: Аудит існуючих AI-рішень**
- Порівняти поточні моделі підприємства з альтернативними механізмами
- Запустити test_practical_utility.py і оцінити: catastrophic forgetting, gradient stability, memory efficiency, length generalization
- Результат: карта вузьких місць поточних рішень

### Проміжні (місяці 1-3)

**Ціль 4: Кастомна модель для свого домену**
- Підготувати корпоративний датасет у форматі parquet
- Тренувати моделі зі всіма 66 конфігураціями (11 attention x 3 optimizer x 2 scheduler)
- Вибрати найкращу комбінацію для конкретної задачі
- Результат: спеціалізована модель, натренована на ваших даних, дані не покидають периметр

**Ціль 5: IP-портфель**
- Документувати результати експериментів
- Якщо tropical attention дає доведену перевагу у fraud detection - це потенційний патент або trade secret
- Результат: задокументовані технічні переваги для конкурентного захисту

**Ціль 6: MLOps-інфраструктура для AI-досліджень**
- На базі артефактів (summary.json + run.md) побудувати систему відстеження експериментів
- Інтегрувати з CI/CD для автоматичних бенчмарків при кожній зміні моделі
- Результат: відтворювана та систематична AI-дослідницька платформа

### Кінцеві (3-12 місяців)

**Ціль 7: Production-deployed спеціалізовані моделі**
- Маленькі моделі з оптимальним attention для конкретних задач замість залежності від зовнішніх API
- Дані залишаються всередині підприємства
- Результат: зменшення витрат на API, повний контроль над даними та моделями

**Ціль 8: Гібридні архітектури**
- Різні attention-типи на різних шарах однієї моделі
- Наприклад: standard (загальне розуміння) -> ultrametric (ієрархія) -> tropical (robust прийняття рішень)
- Результат: архітектура, оптимізована під конкретний домен підприємства

**Ціль 9: Автономна дослідницька платформа**
- Підприємство ставить бізнес-задачу
- Автоматичний pipeline: генерація гіпотез -> бенчмарки -> відбір -> production
- Результат: систематичне перетворення бізнес-проблем на AI-рішення

---

## 6. Чесні обмеження

Що треба розуміти перед початком:

1. **Це дослідницький проект, не production-ready продукт.** Потрібна додаткова робота для hardening, моніторингу, deployment.

2. **Масштабування не доведено.** Переваги показані на маленьких моделях (мільйони параметрів). Чи зберігаються вони на моделях з мільярдами параметрів - відкрите питання.

3. **Екзотичні механізми повільніші.** Tropical, simplicial, octonion attention не мають оптимізованих CUDA-ядер (як FlashAttention для standard). Тому вони повільніші при однаковій кількості параметрів.

4. **Потрібен GPU для серйозних експериментів.** CPU підходить тільки для smoke-тестів та демо. Реальне тренування потребує CUDA-сумісний GPU з 8GB+ VRAM.

5. **FineWeb-Edu - це англомовний датасет.** Для роботи з українськими/мультимовними даними потрібно підготувати свій датасет та, можливо, адаптувати токенізатор.

---

## 7. Сценарій: "Давай підготуй pipeline під наш датасет, що для цього потрібно"

### Крок 1: Підготовка даних

**Що потрібно від вас:**
- Текстові дані підприємства в будь-якому форматі (txt, json, csv, pdf -> конвертований в текст)
- Мінімум: десятки тисяч документів (чим більше - тим краще)
- Дані повинні бути очищені: без HTML-тегів, дублікатів, сміттєвих рядків

**Що ми робимо:**
```
Ваші дані (txt/json/csv)
    |
    v
Скрипт конвертації -> Parquet файли з колонкою "text"
    |
    v
Розділення на шарди: shard_00000.parquet, shard_00001.parquet, ...
    |
    v
Останній шард = validation, решта = training
```

**Формат parquet:**
Кожен файл містить колонку `"text"` з документами. Приклад:

| text |
|------|
| "Звіт про фінансовий стан компанії за Q3 2025..." |
| "Протокол наради від 15.01.2026. Присутні:..." |
| "Технічна специфікація модуля обробки замовлень..." |

**Мінімальні вимоги:**
- Мінімум 2 parquet файли (1 train + 1 val)
- Рекомендовано: 10+ шардів для кращого перемішування

### Крок 2: Токенізатор

**Варіант A (швидкий старт):** Використати GPT-2 токенізатор (вже вбудований). Працює з латиницею та кирилицею, але неоптимальний для української.

**Варіант B (для production):** Навчити свій BPE-токенізатор на ваших даних. Наприклад:
- Vocab size 32000-50000
- Навчити на репрезентативній вибірці вашого корпусу
- Зменшить кількість токенів на документ (= швидше тренування, довший контекст)

### Крок 3: Розміщення даних

```bash
# Дефолтна директорія:
~/.cache/nanochat/base_data/

# Або задати через змінну:
export NANOCHAT_BASE_DIR=/path/to/your/data

# Структура:
/path/to/your/data/base_data/
    shard_00000.parquet
    shard_00001.parquet
    shard_00002.parquet
    ...
```

### Крок 4: Перший тренувальний прогін

```bash
# Smoke test - перевірити що дані завантажуються (CPU, 20 кроків):
python -m nanochat.train \
    --device cpu \
    --max-steps 20 \
    --batch-size 4 \
    --sequence-len 128 \
    --n-layer 2 --n-head 2 --n-kv-head 2 --n-embd 64

# Якщо працює - повноцінний прогін на GPU:
python -m nanochat.train \
    --device cuda \
    --attention-type standard \
    --optimizer-type adamw \
    --batch-size 8 \
    --sequence-len 256 \
    --n-layer 4 --n-head 4 --n-kv-head 4 --n-embd 128 \
    --max-steps 1000 \
    --val-interval 100 \
    --val-batches 20 \
    --learning-rate 6e-4
```

### Крок 5: Систематичний пошук найкращої конфігурації

```bash
# Порівняти 5 найперспективніших attention типів:
mgr bench-fixed-flops \
    -a standard \
    -a tropical \
    -a ultrametric \
    -a simplicial \
    -a quaternion \
    --target-flops 5e9 \
    --device cuda \
    --batch-size 8 \
    --n-layer 4 --n-head 4 --n-embd 128

# Результат: таблиця з final_train_ce для кожного типу
# Далі: для top-3 типів порівняти оптимізатори:
for opt in adamw muon hoss; do
    python -m nanochat.train \
        --attention-type <best-type> \
        --optimizer-type $opt \
        --device cuda \
        --target-flops 5e9 \
        --batch-size 8
done
```

### Крок 6: Аналіз результатів

Після кожного прогону в `artifacts/` з'являються:

- **summary.json** - повна телеметрія (loss на кожному кроці, tokens/s, TFLOP/s, конфігурація)
- **run.md** - людиночитабельний звіт з командою запуску, бюджетом, результатами

Порівнюємо `final_train_ce` (cross-entropy loss) та `final_val_ce` (якщо увімкнена валідація) між конфігураціями.

### Крок 7: Масштабування найкращої конфігурації

Коли знайдена оптимальна комбінація attention + optimizer:

```bash
# Збільшити модель:
python -m nanochat.train \
    --attention-type <winner> \
    --optimizer-type <winner> \
    --device cuda \
    --compile \
    --batch-size 16 \
    --sequence-len 512 \
    --n-layer 8 --n-head 8 --n-kv-head 8 --n-embd 512 \
    --target-flops 1e11 \
    --val-interval 200 \
    --val-batches 50 \
    --learning-rate 3e-4

# Розподілене тренування на кількох GPU:
torchrun --nproc_per_node=4 -m nanochat.train \
    --attention-type <winner> \
    --optimizer-type <winner> \
    --batch-size 8 \
    --sequence-len 512 \
    --n-layer 12 --n-head 12 --n-kv-head 12 --n-embd 768 \
    --target-flops 1e12
```

### Чеклист готовності

- [ ] Текстові дані зібрані та очищені
- [ ] Дані конвертовані в parquet (колонка "text")
- [ ] Розділені на шарди (мінімум 2, рекомендовано 10+)
- [ ] Розміщені в ~/.cache/nanochat/base_data/ або NANOCHAT_BASE_DIR
- [ ] Smoke test на CPU пройдено (loss падає)
- [ ] GPU доступний (nvidia-smi показує карту)
- [ ] Перший повноцінний прогін завершено (standard attention, adamw)
- [ ] Бенчмарк 5+ attention типів завершено
- [ ] Найкраща конфігурація визначена
- [ ] Масштабування розпочато

---

## 8. Технічне середовище

| Компонент | Версія/Вибір | Примітка |
|---|---|---|
| Python | 3.13 | Тільки ця версія |
| Package manager | uv | Ніколи pip |
| ML framework (демо) | JAX | Для інтерактивних досліджень |
| ML framework (prod) | PyTorch | Для тренування моделей |
| CLI | Typer + Rich | Красивий кольоровий вивід |
| Дані | Parquet (PyArrow) | Streaming, ефективна пам'ять |
| Токенізатор | HuggingFace tokenizers (GPT-4 BPE) | Vocab 50304 |
| Проект конфігурація | pyproject.toml | Єдине джерело залежностей |
| Віртуальне середовище | .venv (через uv venv) | Активація через direnv |

---

## 9. Словник термінів

| Термін | Пояснення |
|---|---|
| **Attention** | Механізм, який визначає, на які частини вхідних даних модель "звертає увагу" при обробці кожного елемента |
| **Cross-entropy loss (CE)** | Метрика якості моделі: чим менше - тим краще модель передбачає наступний токен |
| **FLOPs** | Floating Point Operations - кількість обчислень. Фіксований FLOPs-бюджет = чесне порівняння |
| **GQA** | Group-Query Attention: менше KV-голів ніж Q-голів, економить пам'ять |
| **RoPE** | Rotary Position Embeddings: кодує позицію токена через обертання вектора |
| **Softmax** | Функція нормалізації: перетворює числа на ймовірності (сума = 1) |
| **Tokenizer** | Розбиває текст на токени (приблизно: слова або частини слів) |
| **Warmup** | Перші кроки тренування з поступовим збільшенням learning rate |
| **val_ce** | Validation cross-entropy: loss на даних, які модель не бачила при тренуванні |
| **DDP** | Distributed Data Parallel: тренування на кількох GPU одночасно |
| **torch.compile** | JIT-компіляція PyTorch моделі для прискорення |

---

*Документ створено: 2026-02-17*
*Проект: model_guided_research*
*Гілка: training-experiments*
